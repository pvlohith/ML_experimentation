{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('OnlineNewsPopularity.csv') #reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39644 entries, 0 to 39643\n",
      "Data columns (total 61 columns):\n",
      "url                               39644 non-null object\n",
      "timedelta                         39644 non-null int64\n",
      " n_tokens_title                   39644 non-null int64\n",
      " n_tokens_content                 39644 non-null int64\n",
      " n_unique_tokens                  39644 non-null float64\n",
      " n_non_stop_words                 39644 non-null float64\n",
      " n_non_stop_unique_tokens         39644 non-null float64\n",
      " num_hrefs                        39644 non-null int64\n",
      " num_self_hrefs                   39644 non-null int64\n",
      " num_imgs                         39644 non-null int64\n",
      " num_videos                       39644 non-null int64\n",
      " average_token_length             39644 non-null float64\n",
      " num_keywords                     39644 non-null int64\n",
      " data_channel_is_lifestyle        39644 non-null int64\n",
      " data_channel_is_entertainment    39644 non-null int64\n",
      " data_channel_is_bus              39644 non-null int64\n",
      " data_channel_is_socmed           39644 non-null int64\n",
      " data_channel_is_tech             39644 non-null int64\n",
      " data_channel_is_world            39644 non-null int64\n",
      " kw_min_min                       39644 non-null int64\n",
      " kw_max_min                       39644 non-null float64\n",
      " kw_avg_min                       39644 non-null float64\n",
      " kw_min_max                       39644 non-null int64\n",
      " kw_max_max                       39644 non-null int64\n",
      " kw_avg_max                       39644 non-null float64\n",
      " kw_min_avg                       39644 non-null float64\n",
      " kw_max_avg                       39644 non-null float64\n",
      " kw_avg_avg                       39644 non-null float64\n",
      " self_reference_min_shares        39644 non-null float64\n",
      " self_reference_max_shares        39644 non-null float64\n",
      " self_reference_avg_sharess       39644 non-null float64\n",
      " weekday_is_monday                39644 non-null int64\n",
      " weekday_is_tuesday               39644 non-null int64\n",
      " weekday_is_wednesday             39644 non-null int64\n",
      " weekday_is_thursday              39644 non-null int64\n",
      " weekday_is_friday                39644 non-null int64\n",
      " weekday_is_saturday              39644 non-null int64\n",
      " weekday_is_sunday                39644 non-null int64\n",
      " is_weekend                       39644 non-null int64\n",
      " LDA_00                           39644 non-null float64\n",
      " LDA_01                           39644 non-null float64\n",
      " LDA_02                           39644 non-null float64\n",
      " LDA_03                           39644 non-null float64\n",
      " LDA_04                           39644 non-null float64\n",
      " global_subjectivity              39644 non-null float64\n",
      " global_sentiment_polarity        39644 non-null float64\n",
      " global_rate_positive_words       39644 non-null float64\n",
      " global_rate_negative_words       39644 non-null float64\n",
      " rate_positive_words              39644 non-null float64\n",
      " rate_negative_words              39644 non-null float64\n",
      " avg_positive_polarity            39644 non-null float64\n",
      " min_positive_polarity            39644 non-null float64\n",
      " max_positive_polarity            39644 non-null float64\n",
      " avg_negative_polarity            39644 non-null float64\n",
      " min_negative_polarity            39644 non-null float64\n",
      " max_negative_polarity            39644 non-null float64\n",
      " title_subjectivity               39644 non-null float64\n",
      " title_sentiment_polarity         39644 non-null float64\n",
      " abs_title_subjectivity           39644 non-null float64\n",
      " abs_title_sentiment_polarity     39644 non-null float64\n",
      "shares                            39644 non-null int64\n",
      "dtypes: float64(34), int64(26), object(1)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 . Removing non predictive attributes and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shares=df['shares']\n",
    "features=df.drop(['url','timedelta','shares'],axis=1)\n",
    "features = (features - features.mean()) / (features.max() - features.min()) #Normalisation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into test and train\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, shares, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Linear Regression on test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model1= LinearRegression()\n",
    "model1.fit(X_train,y_train)\n",
    "y_predict_train=model1.predict(X_train)\n",
    "y_predict_test=model1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score #performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022708408923445167\n"
     ]
    }
   ],
   "source": [
    "r2score_train=r2_score(y_train,y_predict_train)\n",
    "print(r2score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01949173984149566\n"
     ]
    }
   ],
   "source": [
    "r2score_test=r2_score(y_test,y_predict_test)\n",
    "print(r2score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3: Converting shares into a binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of binary variable for shares based on median.\n",
    "median_shares=np.median(shares)\n",
    "shares_ind=[]\n",
    "for share in shares:\n",
    "    if share>= median_shares:\n",
    "        shares_ind.append(1)\n",
    "    else:\n",
    "        shares_ind.append(0)\n",
    "shares_binary=pd.DataFrame(shares_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4: Logistic Regressionon train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvloh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Splitting data into test and train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, shares_binary, test_size=0.3, random_state=100)\n",
    "model2= LogisticRegression()\n",
    "model2.fit(X_train,y_train)\n",
    "y_predict_train=model2.predict(X_train)\n",
    "y_predict_test=model2.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score #Performance metric for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646990990990991\n"
     ]
    }
   ],
   "source": [
    "accuracy_train=accuracy_score(y_train,y_predict_train)\n",
    "print(accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646628552211199\n"
     ]
    }
   ],
   "source": [
    "accuracy_test=accuracy_score(y_test,y_predict_test)\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features['inter']=np.ones((df.shape[0]), dtype=np.int) #Creating a column with zeros to calculate intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cost function for gradient descent linear regression\n",
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        loss = hypothesis - y\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        #print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numIterations= 1000\n",
    "alpha = 0.005\n",
    "m, n = np.shape(features)\n",
    "theta = np.ones(n)\n",
    "theta = gradientDescent(features, shares, theta, alpha, m, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00971327369544861"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=np.dot(features,theta.transpose())\n",
    "score=r2_score(shares,y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.020576112248769762 0.017248173242646825\n",
      "0.3 0.01878044744516394 0.016025412375853865\n",
      "0.1 0.016479621826107294 0.014776966546349635\n",
      "0.05 0.01510438252841606 0.013969497180605361\n",
      "0.01 0.012023880651999885 0.011568591730339639\n",
      "0.005 0.009630518886600825 0.009259297326538518\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with the above cost function on train and test data with alpha=0.3,0.1,0.01,0.05\n",
    "\n",
    "alpha_values=[1,0.3,0.1,0.05,0.01,0.005]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, shares, test_size=0.3, random_state=100)\n",
    "for alpha in alpha_values:\n",
    "        m, n = np.shape(X_train)\n",
    "        theta = np.ones(n)\n",
    "        theta = gradientDescent(X_train, y_train, theta, alpha, m, 1000)\n",
    "        y_pred_train=np.dot(X_train,theta.transpose())\n",
    "        y_pred_test=np.dot(X_test,theta.transpose())\n",
    "        score_train=r2_score(y_train,y_pred_train)\n",
    "        score_test=r2_score(y_test,y_pred_test)\n",
    "        print(alpha, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cost function for gradient descent logistic regression\n",
    "def sigmoid(x):\n",
    "    f = 1.0/(1.0 + np.exp(-x))\n",
    "    return f\n",
    "def logistic_reg(x, y, theta, alpha, m, Iterations):\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, Iterations):\n",
    "        hypothesis = sigmoid(np.dot(x, theta))\n",
    "        cost = np.sum( -y[0]*hypothesis - (1-y[0])*(np.log(1-hypothesis)) )/m\n",
    "        #print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        gradient = np.dot(xTrans,hypothesis-y[0]) / m\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99608099,  0.99205797,  0.99998826,  0.9999703 ,  0.9999782 ,\n",
       "        0.99488851,  0.99587325,  0.99410136,  0.99764963,  0.97832266,\n",
       "        0.98402177,  0.98919725,  0.98659852,  0.9896444 ,  0.99283891,\n",
       "        0.97416342,  1.03014262,  1.01442153,  0.99956805,  0.99967678,\n",
       "        0.99382137,  0.96760858,  0.98761348,  0.93864026,  0.99874416,\n",
       "        0.99651687,  0.99898508,  0.99622764,  0.99804989,  1.01372188,\n",
       "        1.01229212,  1.01611379,  1.01406533,  1.0120251 ,  0.96767886,\n",
       "        0.96410292,  0.93178178,  0.98882778,  1.00329461,  1.02157991,\n",
       "        1.00908738,  0.97737844,  0.97213602,  0.97449017,  0.97380187,\n",
       "        0.99991616,  0.95070344,  1.01072742,  0.96833179,  0.99492168,\n",
       "        0.92129353,  0.99070172,  0.98746882,  0.99247906,  0.93895079,\n",
       "        0.9781515 ,  0.9910545 ,  0.95246172, -0.58087019])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Iterations= 100\n",
    "alpha = 0.03\n",
    "m, n = np.shape(features)\n",
    "theta = np.ones(n)\n",
    "y=shares_binary\n",
    "x=features\n",
    "theta = logistic_reg(x, np.array(y), theta, alpha,m, Iterations)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.536096256684492"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=np.dot(features,theta.transpose())\n",
    "y_predicted=[]\n",
    "for i in y_pred:\n",
    "    value=sigmoid(i)\n",
    "    if value>=0.5:\n",
    "        y_predicted.append(1)\n",
    "    else:\n",
    "        y_predicted.append(0)\n",
    "y_predicted=np.array(y_predicted)\n",
    "score=accuracy_score(shares_binary,y_predicted)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.46645045045045047 0.46628552211198926\n",
      "0.3 0.46645045045045047 0.46628552211198926\n",
      "0.1 0.46645045045045047 0.46628552211198926\n",
      "0.05 0.4664864864864865 0.46628552211198926\n",
      "0.01 0.47549549549549547 0.4750294266016479\n",
      "0.005 0.5033513513513513 0.5011770640659156\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with the above cost function on train and test data with alpha=0.3,0.1,0.01,0.05\n",
    "alpha_values=[1,0.3,0.1,0.05,0.01,0.005]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, shares_binary, test_size=0.3, random_state=25)\n",
    "for alpha in alpha_values:\n",
    "        m, n = np.shape(X_train)\n",
    "        theta = np.ones(n)\n",
    "        theta = logistic_reg(X_train, np.array(y_train), theta, alpha,m, 1000)\n",
    "        y_pred_train=np.dot(X_train,theta.transpose())\n",
    "        y_pred_test=np.dot(X_test,theta.transpose())\n",
    "        y_predicted_train=[]\n",
    "        y_predicted_test=[]\n",
    "        for i in y_pred_train:\n",
    "            value=sigmoid(i)\n",
    "            if value>=0.5:\n",
    "                y_predicted_train.append(1)\n",
    "            else:\n",
    "                y_predicted_train.append(0)\n",
    "        for i in y_pred_test:\n",
    "            value=sigmoid(i)\n",
    "            if value>=0.5:\n",
    "                y_predicted_test.append(1)\n",
    "            else:\n",
    "                y_predicted_test.append(0)\n",
    "        y_predicted_train=np.array(y_predicted_train)\n",
    "        y_predicted_test=np.array(y_predicted_test)\n",
    "        score_train=accuracy_score(y_train,y_predicted_train)\n",
    "        score_test=accuracy_score(y_test,y_predicted_test)\n",
    "        print(alpha, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvloh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Experiment2: Selecting 10 random columns and running the model(selecting the first 10 columns)\n",
    "random_slice= features.iloc[:,[2,44,51,10,9,5,18,33,27,40]]\n",
    "random_slice['inter']=np.ones((df.shape[0]), dtype=np.int) #Creating a column with zeros to calculate intercept\n",
    "#Based on the above experimentation the best learning rate is 0.3 for linear regression and 0.005 for logistic\n",
    "#Running linear and logistic models based on this learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.010242716276614994 0.010386279797535924\n"
     ]
    }
   ],
   "source": [
    "#Linear model\n",
    "alpha_values=[0.3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(random_slice, shares, test_size=0.3, random_state=100)\n",
    "for alpha in alpha_values:\n",
    "        m, n = np.shape(X_train)\n",
    "        theta = np.ones(n)\n",
    "        theta = gradientDescent(X_train, y_train, theta, alpha, m, 1000)\n",
    "        y_pred_train=np.dot(X_train,theta.transpose())\n",
    "        y_pred_test=np.dot(X_test,theta.transpose())\n",
    "        score_train=r2_score(y_train,y_pred_train)\n",
    "        score_test=r2_score(y_test,y_pred_test)\n",
    "        print(alpha, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 0.4720720720720721 0.4715823104086094\n"
     ]
    }
   ],
   "source": [
    "#Logistic Model\n",
    "alpha_values=[0.005]\n",
    "X_train, X_test, y_train, y_test = train_test_split(random_slice, shares_binary, test_size=0.3, random_state=25)\n",
    "for alpha in alpha_values:\n",
    "        m, n = np.shape(X_train)\n",
    "        theta = np.ones(n)\n",
    "        theta = logistic_reg(X_train, np.array(y_train), theta, alpha,m, 1000)\n",
    "        y_pred_train=np.dot(X_train,theta.transpose())\n",
    "        y_pred_test=np.dot(X_test,theta.transpose())\n",
    "        y_predicted_train=[]\n",
    "        y_predicted_test=[]\n",
    "        for i in y_pred_train:\n",
    "            value=sigmoid(i)\n",
    "            if value>=0.5:\n",
    "                y_predicted_train.append(1)\n",
    "            else:\n",
    "                y_predicted_train.append(0)\n",
    "        for i in y_pred_test:\n",
    "            value=sigmoid(i)\n",
    "            if value>=0.5:\n",
    "                y_predicted_test.append(1)\n",
    "            else:\n",
    "                y_predicted_test.append(0)\n",
    "        y_predicted_train=np.array(y_predicted_train)\n",
    "        y_predicted_test=np.array(y_predicted_test)\n",
    "        score_train=accuracy_score(y_train,y_predicted_train)\n",
    "        score_test=accuracy_score(y_test,y_predicted_test)\n",
    "        print(alpha, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentaion 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' n_tokens_title', ' n_tokens_content', ' n_unique_tokens',\n",
       "       ' n_non_stop_words', ' n_non_stop_unique_tokens', ' num_hrefs',\n",
       "       ' num_self_hrefs', ' num_imgs', ' num_videos', ' average_token_length',\n",
       "       ' num_keywords', ' data_channel_is_lifestyle',\n",
       "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
       "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
       "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
       "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
       "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
       "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
       "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
       "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
       "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
       "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
       "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
       "       ' global_rate_negative_words', ' rate_positive_words',\n",
       "       ' rate_negative_words', ' avg_positive_polarity',\n",
       "       ' min_positive_polarity', ' max_positive_polarity',\n",
       "       ' avg_negative_polarity', ' min_negative_polarity',\n",
       "       ' max_negative_polarity', ' title_subjectivity',\n",
       "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
       "       ' abs_title_sentiment_polarity', 'inter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running regression model on 10 best chosen features\n",
    "# Features selected are -  n_tokens_content,  num_imgs, weekday_is_friday,weekday_is_saturday, weekday_is_sunday, global_subjectivity\n",
    "## global_rate_positive_words, title_sentiment_polarity, data_channel_is_socmed\t, self_reference_max_shares\n",
    "features.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvloh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "select_data=features.iloc[:,[1,7,33,34,35,42,44,55,14,27]]\n",
    "select_data['inter']=np.ones((df.shape[0]), dtype=np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.004504066560499953 0.004128715437329067\n"
     ]
    }
   ],
   "source": [
    "#Linear model\n",
    "alpha_values=[0.3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(select_data, shares, test_size=0.3, random_state=100)\n",
    "for alpha in alpha_values:\n",
    "        m, n = np.shape(X_train)\n",
    "        theta = np.ones(n)\n",
    "        theta = gradientDescent(X_train, y_train, theta, alpha, m, 1000)\n",
    "        y_pred_train=np.dot(X_train,theta.transpose())\n",
    "        y_pred_test=np.dot(X_test,theta.transpose())\n",
    "        score_train=r2_score(y_train,y_pred_train)\n",
    "        score_test=r2_score(y_test,y_pred_test)\n",
    "        print(alpha, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 0.4803243243243243 0.4815873549688919\n"
     ]
    }
   ],
   "source": [
    "#Logistic Model\n",
    "alpha_values=[0.005]\n",
    "X_train, X_test, y_train, y_test = train_test_split(select_data, shares_binary, test_size=0.3, random_state=25)\n",
    "for alpha in alpha_values:\n",
    "        m, n = np.shape(X_train)\n",
    "        theta = np.ones(n)\n",
    "        theta = logistic_reg(X_train, np.array(y_train), theta, alpha,m, 1000)\n",
    "        y_pred_train=np.dot(X_train,theta.transpose())\n",
    "        y_pred_test=np.dot(X_test,theta.transpose())\n",
    "        y_predicted_train=[]\n",
    "        y_predicted_test=[]\n",
    "        for i in y_pred_train:\n",
    "            value=sigmoid(i)\n",
    "            if value>=0.5:\n",
    "                y_predicted_train.append(1)\n",
    "            else:\n",
    "                y_predicted_train.append(0)\n",
    "        for i in y_pred_test:\n",
    "            value=sigmoid(i)\n",
    "            if value>=0.5:\n",
    "                y_predicted_test.append(1)\n",
    "            else:\n",
    "                y_predicted_test.append(0)\n",
    "        y_predicted_train=np.array(y_predicted_train)\n",
    "        y_predicted_test=np.array(y_predicted_test)\n",
    "        score_train=accuracy_score(y_train,y_predicted_train)\n",
    "        score_test=accuracy_score(y_test,y_predicted_test)\n",
    "        print(alpha, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
